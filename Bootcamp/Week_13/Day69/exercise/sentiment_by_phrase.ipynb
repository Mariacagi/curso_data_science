{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/\n",
    "\n",
    "En el anterior enlace, tenéis un ejemplo sobre cómo, a partir de tweets con un label específico (un sentimiento, positivo o negativo): \n",
    "\n",
    "1. Genera un conjunto de entrenamiento. El conjunto de entrenamiento es formado a partir de tweets completos pasados a un array con un tamaño específico.\n",
    "2. Ese array (X_train de tamaño N) tiene un label que representa el sentimiento (y_train)\n",
    "3. Como todas las frases tienen un tamaño N, la entrada de la red neuronal será de tamaño N y la salida de la red será de tamaño 2 usando activación softmax(porque hay dos clases).\n",
    "\n",
    "Se pide: \n",
    "\n",
    "- Realizar un clasificador de reviews para el dataset de IMDB de la carpeta data_exercise/\n",
    "\n",
    "**Cuando usa la importación \"keras.x\", reemplázalo por \"tensorflow.keras.x\"**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\"\"\"\n",
    "De\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "Usa\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import re \n",
    "import string \n",
    "import json\n",
    "\n",
    "\n",
    "import tensorflow.keras.preprocessing.text as kpt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../exercise/data_exercise/IMDB_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "## Hacemos label encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['sentiment']"
   ]
  },
  {
   "source": [
    "le = LabelEncoder()\n",
    "sentiment_le = le.fit_transform(y)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              review sentiment  sentiment_le\n",
       "0  One of the other reviewers has mentioned that ...  positive             1\n",
       "1  A wonderful little production. <br /><br />The...  positive             1\n",
       "2  I thought this was a wonderful way to spend ti...  positive             1\n",
       "3  Basically there's a family where a little boy ...  negative             0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive             1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>sentiment_le</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "df[\"sentiment_le\"] = sentiment_le\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "## Hacemos split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"review\"]\n",
    "y = df[\"sentiment_le\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "source": [
    "## Quitamos los símbolos del texto"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000,), dtype=string, numpy=\n",
       "array([b'endearingly silly anime only six episodes in duration about a hapless delivery boy called kintaro well hes called a delivery boy though he is meant to be in his 20s and the adventures he has on his travels each episode sees him arriving in a new town acquiring a new job developing something of a love interest before each episode ends with him leaving  gently sexist juvenile very immature at times this is the kind of anime that just puts a smile on the face  not one to start with if you are not a fan of anime as this certainly wont convince you about the genre but for those who are already converted this is entertaining fluff',\n",
       "       b'while a 9 might seem like an unusually high score for such a slight film however compared to the hundreds and hundreds of series detective films from the 1930s and 40s this is among the very best and also compares very favorably to powells later thin man films now this does not mean that the film is that similar to the thin man movies as the kennel murder case is not a comedy but more a traditional mysterydetective film now youd think that not having nora charles or asta or a traditional comic sidekick something found in practically all series detective films along for fun would be a detriment but i didnt miss them at all because this was such an exceptionally wellwritten filmhaving a genuinely interesting case as well as uniformly excellent performances by all  the film begins at the dog show and is called the kennel murder case though this philo vance film actually spends little of the time at the dog show and dogs are not a superimportant part of the film instead a thoroughly hated man is killed and left in a completely sealed rooman idea repeated in quite a few other detective films such as crime doctors strangest case however how all this is explained seems pretty credible and fit together very wellkeeping my interest throughout i sure wish other detective films of the day had as intelligently written plots and exceptional acting as this one this one is definitely a keeper',\n",
       "       b'without a doubt 12 monkeys is one of the best films of the scifi genre and director terry gilliam is no stranger at pulling off such cinematic originality an apocalyptic film that holds you completely spellbound 12 monkeys never lets up and has you guessing all the way throughout excellent use of philadelphia locales and netherworld sets create a gothic sense of tragedy and two people caught in time at the wrong place  bruce willis escapes his macho image and portrays a true loony who happens to be right about all that will happen he is actually sane but the people of the future or present if you will distort this guys head so bad through time travel no wonder he unravels he gets sent to world war i just after beng sent to the wrong year to find out how the army of the twelve monkeys pulls off the annihilation of civilization as we know it they finally get it right and in what is truly a remarkable screenplay to match the performance we get to see willis madeleine stowe and an ominous brad pitt crossreferenced over the course of 6 years  stowe is sensual and solid as the risktaking shrink who slowly starts to realize that willis may not be as cracked up as he seems a captivating element of the relationship between her and willis is their sense of seeing each other before in another place or time 12 monkeys is essentially about time and the madness the futuristic people immerse into it and the times of the present when killers and a psychotic genius can alter the world  the brooding city of philadelphia is a dark and gothic backdrop for willis plight to complete his mission which is against all usual hollywood stereotype not to save the world he is gathering information the film plays tricks on the viewer as well placing willis in a new setting at the drop of a pin this must have been an extremely difficult picture to make but gilliam seems to be the master of hardboiled movie making he even drops in some humor reminiscent of other great works like time bandits and brazil the screen is this mans canvas and he knows how to paint a sometimes terrifying picture of the world and its possible future within the mainstream atmosphere of bigbudget films if you want sincere madness and ironic tragedy see 12 monkeys  rating 9 of 10',\n",
       "       ...,\n",
       "       b'a very funny movie it was good to see jim carrey back in top form it was definitely worth the price of admission morgan freeman and jennifer aniston both played outstanding supporting roles in this film i think they may have played the dog a bit too much however still a good film to see',\n",
       "       b'this is one of the worst sandra bullock movie since speed 2 but not quite that bad i really lost it with those out of the blue not so special effect guys if youre an insomniac go with your girl to see this movie i give it three sleepies',\n",
       "       b'it helps if you understand czech and can see this in the original language and understand the czechs obsession with the professionals but if not jedna ruka netlaska is yet another great czech film it is funny dark and extremely enjoyable the highest compliment i can pay it is that you never know quite what is going to happen next and even keep that feeling well into the second and third viewing  for a small country the czech republic has produced an amazing amount of world class film and literature from hrabal hasek and kundera to the films of menzel sverak and numerous others czech humour by its very nature is dark and often uncompromising but often with a naive and warm sentiment behind it this film is just that it is unkind and deals with the less lovable sides of human beings but underneath it all there is a beautiful story full of promise good intent and optimism  i highly recommend this and most other projects trojan and machacek are involved in enjoy it its a film made for just that reason  anyway its as close as the czechs will ever come to writing a truly happy ending'],\n",
       "      dtype=object)>"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "# quitamos tags de HTML '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data) #le pasa los strings a minuscula\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')# le quita el br (salto de linea en html) \n",
    "  return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation), '') \n",
    "\n",
    "custom_standardization(input_data=X_train)\n",
    "custom_standardization(input_data=X_test)"
   ]
  },
  {
   "source": [
    "## Hacemos el tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tamaño máximo de palabras y Vocabulary size and number of words in a sequence:\n",
    "max_words = 1000\n",
    "sequence_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)# create a new Tokenizer\n",
    "tokenizer.fit_on_texts(X_train)# feed our tweets to the Tokenizer\n",
    "dictionary = tokenizer.word_index # Tokenizers come with a convenient list of words and IDs"
   ]
  },
  {
   "source": [
    "## Guardamos el tokenizer/diccionary en un json"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)"
   ]
  },
  {
   "source": [
    "## Convertimos el texto a array con el tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does is make all texts the same length -- in this case, the length of the longest text in the set.\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWordIndices = []\n",
    "# for each review, change each token to its ID in the Tokenizer's word_index\n",
    "for text in X_train:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a list of all review converted to index arrays. Cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "X_train = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "source": [
    "## Creamos el modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "## Entrenamos el modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1125/1125 [==============================] - 5s 4ms/step - loss: 0.4592 - accuracy: 0.7735 - val_loss: 0.3339 - val_accuracy: 0.8533\n",
      "Epoch 2/5\n",
      "1125/1125 [==============================] - 4s 4ms/step - loss: 0.3121 - accuracy: 0.8665 - val_loss: 0.3329 - val_accuracy: 0.8535\n",
      "Epoch 3/5\n",
      "1125/1125 [==============================] - 5s 4ms/step - loss: 0.2863 - accuracy: 0.8772 - val_loss: 0.3340 - val_accuracy: 0.8543\n",
      "Epoch 4/5\n",
      "1125/1125 [==============================] - 4s 4ms/step - loss: 0.2380 - accuracy: 0.8968 - val_loss: 0.3463 - val_accuracy: 0.8553\n",
      "Epoch 5/5\n",
      "1125/1125 [==============================] - 5s 4ms/step - loss: 0.1888 - accuracy: 0.9204 - val_loss: 0.3770 - val_accuracy: 0.8565\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20974026160>"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "        batch_size=32, #data in groups of batch_size\n",
    "        epochs=5, #epochs is how many times you do this batch-by-batch splitting. I’ve found 5 to be good in this  case; I tried 7, but ended up overfitting.\n",
    "        verbose=1,\n",
    "        validation_split=0.1,\n",
    "        shuffle=True)"
   ]
  },
  {
   "source": [
    "## Guardamos el modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "source": [
    "## Leemos el modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our saved dictionary\n",
    "with open('dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "    return wordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in your saved model structure\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('model.h5')"
   ]
  },
  {
   "source": [
    "## Parte interactiva"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Good movie\n",
      "positive sentiment; 57.277578% confidence\n",
      "I hate it\n",
      "negative sentiment; 63.744241% confidence\n",
      "I love it\n",
      "positive sentiment; 96.705806% confidence\n",
      "Really bad\n",
      "negative sentiment; 99.362791% confidence\n",
      "Really good\n",
      "positive sentiment; 83.517128% confidence\n"
     ]
    }
   ],
   "source": [
    "# for human-friendly printing\n",
    "labels = ['negative', 'positive']\n",
    "\n",
    "while 1:\n",
    "    evalSentence = input('Input a sentence to be evaluated, or Enter to quit: ')\n",
    "\n",
    "    if len(evalSentence) == 0:\n",
    "        break\n",
    " \n",
    "    # format your input for the neural net\n",
    "    testArr = convert_text_to_index_array(evalSentence)\n",
    "  \n",
    "    input_f = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "\n",
    "    # predict which bucket your input belongs in\n",
    "    pred = model.predict(input_f)\n",
    "    # and print it for the humons\n",
    "    print(evalSentence)\n",
    "    print(\"%s sentiment; %f%% confidence\" % (labels[np.argmax(pred)], pred[0][np.argmax(pred)] * 100))"
   ]
  }
 ]
}