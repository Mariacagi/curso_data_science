{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "af698c02722c81864863429d928a3bd8a8dcc030c4156359d3ef8b980fb01319"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. From HTML\n",
    "\n",
    "*Using only beautiful soap, no regex*\n",
    "\n",
    "Save in a dataframe the next information using web scraping. Each row of the dataframe must have in different columns:\n",
    "\n",
    "- The name of the title\n",
    "- The id of the div where is the value scraped. If there is not id, then the value is must be numpy.nan\n",
    "- The name of the tag where is the value scraped.\n",
    "- The next scraped values in different rows: \n",
    "    - The value: \"Este es el segundo párrafo\"  --> Row 1\n",
    "    - The url https://pagina1.xyz/ --> Row 2\n",
    "    - The url https://pagina4.xyz/ --> Row 3\n",
    "    - The url https://pagina5.xyz/ --> Row 4\n",
    "    - The value \"links footer-links\" --> Row 5\n",
    "    - The value \"Este párrafo está en el footer\" --> Row 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Página de prueba</title>\n",
    "</head>\n",
    "<body>\n",
    "<div id=\"main\" class=\"full-width\">\n",
    "    <h1>El título de la página</h1>\n",
    "    <p>Este es el primer párrafo</p>\n",
    "    <p>Este es el segundo párrafo</p>\n",
    "    <div id=\"innerDiv\">\n",
    "        <div class=\"links\">\n",
    "            <a href=\"https://pagina1.xyz/\">Enlace 1</a>\n",
    "            <a href=\"https://pagina2.xyz/\">Enlace 2</a>\n",
    "        </div>\n",
    "        <div class=\"right\">\n",
    "            <div class=\"links\">\n",
    "                <a href=\"https://pagina3.xyz/\">Enlace 3</a>\n",
    "                <a href=\"https://pagina4.xyz/\">Enlace 4</a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div id=\"footer\">\n",
    "        <!-- El footer -->\n",
    "        <p>Este párrafo está en el footer</p>\n",
    "        <div class=\"links footer-links\">\n",
    "            <a href=\"https://pagina5.xyz/\">Enlace 5</a>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "source": [
    "## Sacamos el html ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Página de prueba\n\n\n\nEl título de la página\nEste es el primer párrafo\nEste es el segundo párrafo\n\n\nEnlace 1\nEnlace 2\n\n\n\nEnlace 3\nEnlace 4\n\n\n\n\n\nEste párrafo está en el footer\n\nEnlace 5\n\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "def show_html(html_str):\n",
    "    print(BeautifulSoup(str(html_str), 'html.parser').prettify())\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "show_html(soup.text)"
   ]
  },
  {
   "source": [
    "## LINKS URL ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<a href=\"https://pagina1.xyz/\">Enlace 1</a>,\n",
       " <a href=\"https://pagina2.xyz/\">Enlace 2</a>,\n",
       " <a href=\"https://pagina3.xyz/\">Enlace 3</a>,\n",
       " <a href=\"https://pagina4.xyz/\">Enlace 4</a>,\n",
       " <a href=\"https://pagina5.xyz/\">Enlace 5</a>]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "#Etiqueta A\n",
    "links = soup.findAll('a')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://pagina1.xyz/\nhttps://pagina4.xyz/\nhttps://pagina5.xyz/\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://pagina1.xyz/', 'https://pagina4.xyz/', 'https://pagina5.xyz/']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "#Etiqueta href\n",
    "\n",
    "list_of_links = []\n",
    "acum = 0\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    if href and href[:4] == \"http\" and acum == 0:\n",
    "        print(href)\n",
    "        list_of_links.append(href)\n",
    "\n",
    "    elif href and href[:4] == \"http\" and acum == 3:\n",
    "        print(href)\n",
    "        list_of_links.append(href)\n",
    "    \n",
    "    elif href and href[:4] == \"http\" and acum == 4:\n",
    "        print(href)\n",
    "        list_of_links.append(href)\n",
    "\n",
    "    acum += 1\n",
    "\n",
    "list_of_links"
   ]
  },
  {
   "source": [
    "## Segundo párrafo y Footer párrafo ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<p>Este es el primer párrafo</p>,\n",
       " <p>Este es el segundo párrafo</p>,\n",
       " <p>Este párrafo está en el footer</p>]"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "#Etiqueta p\n",
    "todos_los_parrafos = soup.findAll('p')\n",
    "todos_los_parrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Este es el segundo párrafo']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Este párrafo está en el footer']"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "#Cogemos los párrafos segundo y footer\n",
    "acum = 0\n",
    "for p in todos_los_parrafos:\n",
    "    acum +=1\n",
    "    if acum >1 and acum <3:\n",
    "        segundo_parrafo = p.contents\n",
    "    if acum >2 and acum <4:\n",
    "        parrafo_footer = p.contents\n",
    "\n",
    "print(segundo_parrafo)\n",
    "parrafo_footer"
   ]
  },
  {
   "source": [
    "## Class Footer ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<div class=\"links footer-links\">\n",
      " <a href=\"https://pagina5.xyz/\">\n",
      "  Enlace 5\n",
      " </a>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "#Etiqueta class\n",
    "footer_links = soup.find(class_='links footer-links')\n",
    "show_html(footer_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['links', 'footer-links']"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "#Cogemos lo que hay dentro de class\n",
    "clasecita = footer_links.get('class')\n",
    "clasecita"
   ]
  },
  {
   "source": [
    "## Los values ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Este es el segundo párrafo']\n['https://pagina1.xyz/', 'https://pagina4.xyz/', 'https://pagina5.xyz/']\n['Este párrafo está en el footer']\n['links', 'footer-links']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Este es el segundo párrafo',\n",
       " 'https://pagina1.xyz/',\n",
       " 'https://pagina4.xyz/',\n",
       " 'https://pagina5.xyz/',\n",
       " ['links', 'footer-links'],\n",
       " 'Este párrafo está en el footer']"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "print(segundo_parrafo)\n",
    "print(list_of_links)\n",
    "print(parrafo_footer)\n",
    "print(clasecita)\n",
    "\n",
    "Values = [segundo_parrafo[0], list_of_links[0], list_of_links[1], list_of_links[2], clasecita, parrafo_footer[0]]\n",
    "Values"
   ]
  },
  {
   "source": [
    "## El título ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['Página de prueba'],\n",
       " ['Página de prueba'],\n",
       " ['Página de prueba'],\n",
       " ['Página de prueba'],\n",
       " ['Página de prueba'],\n",
       " ['Página de prueba']]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "#Etiqueta title\n",
    "\n",
    "parrafo = soup.find('title')\n",
    "title_parrafo = parrafo.contents\n",
    "title_parrafo\n",
    "\n",
    "#Longitud de title tantas veces como Values haya\n",
    "title2 = []\n",
    "\n",
    "for x in range(len(Values)):\n",
    "    title2.append(title_parrafo)\n",
    "\n",
    "title2"
   ]
  },
  {
   "source": [
    "## Las etiquetas ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"title\", \"a\", \"a\", \"a\", \"class\", \"p\"]"
   ]
  },
  {
   "source": [
    "## Los id ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['main', 'innerDiv', 'footer']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['main', nan, nan, nan, nan, 'footer']"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "ids = []\n",
    "for id in soup.select(\"div[id]\"):\n",
    "    ids.append(id[\"id\"])\n",
    "\n",
    "print(ids) \n",
    "\n",
    "ids_lista = [ids[0], numpy.nan, numpy.nan, numpy.nan, numpy.nan, ids[2]]\n",
    "ids_lista"
   ]
  },
  {
   "source": [
    "## El dataframe ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                Title      id   tags                          Values\n",
       "0  [Página de prueba]    main  title      Este es el segundo párrafo\n",
       "1  [Página de prueba]     NaN      a            https://pagina1.xyz/\n",
       "2  [Página de prueba]     NaN      a            https://pagina4.xyz/\n",
       "3  [Página de prueba]     NaN      a            https://pagina5.xyz/\n",
       "4  [Página de prueba]     NaN  class           [links, footer-links]\n",
       "5  [Página de prueba]  footer      p  Este párrafo está en el footer"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>id</th>\n      <th>tags</th>\n      <th>Values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[Página de prueba]</td>\n      <td>main</td>\n      <td>title</td>\n      <td>Este es el segundo párrafo</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[Página de prueba]</td>\n      <td>NaN</td>\n      <td>a</td>\n      <td>https://pagina1.xyz/</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[Página de prueba]</td>\n      <td>NaN</td>\n      <td>a</td>\n      <td>https://pagina4.xyz/</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[Página de prueba]</td>\n      <td>NaN</td>\n      <td>a</td>\n      <td>https://pagina5.xyz/</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[Página de prueba]</td>\n      <td>NaN</td>\n      <td>class</td>\n      <td>[links, footer-links]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[Página de prueba]</td>\n      <td>footer</td>\n      <td>p</td>\n      <td>Este párrafo está en el footer</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "dict_df = {\"Title\": title2, \"id\": ids_lista, \"tags\" : tags, \"Values\": Values}\n",
    "\n",
    "df = pd.DataFrame(dict_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. From Amazon\n",
    "\n",
    "*Using  beautiful soap  (regex optional)*\n",
    "\n",
    "Save in a dataframe the next information using web scraping. Using product pages from Amazon, do the following: \n",
    "\n",
    "- Get the product name from the web and save it in a column called \"item_name\"\n",
    "- Get the price from the web and save it in a column called \"item_price\"\n",
    "\n",
    "While you are doing the exercise, document the steps you are doing. Try to do the program for generic pages. If you cannot do it generic, explain the reasons. \n",
    "\n",
    "*We recommend to get the source-code, save it in a local file and work from there. It is possible that Amazon detects that you are using webscraping and it changes the source code to avoid possibles attacks.*\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "**Example:** \n",
    "\n",
    "url = https://www.amazon.es/Tommy-Hilfiger-UM0UM00054-Camiseta-Hombre/dp/B01MYD0T1F/ref=sr_1_1?dchild=1&pf_rd_p=58224bec-cac9-4dd2-a42a-61b1db609c2d&pf_rd_r=VZQ1JTQXFVRZ9E9VSKX4&qid=1595364419&s=apparel&sr=1-1\n",
    "\n",
    "*item_name* --> \"Tommy Hilfiger Logo Camiseta de Cuello Redondo,Perfecta para El Tiempo Libre para Hombre\"\n",
    "\n",
    "*item_price* --> [[18,99 € - 46,59 €]] or one of the options.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}